{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training the adverserial debiaser\n",
        "\n"
      ],
      "metadata": {
        "id": "qB-zA8TI_mc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading the packages\n",
        "!pip install aif360\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.datasets import StandardDataset\n",
        "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.metrics import ClassificationMetric\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from google.colab import drive\n",
        "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
        "import tensorflow.compat.v1 as tf\n",
        "import joblib\n",
        "import cloudpickle\n",
        "import pickle\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# mount collab to drive \n",
        "drive.mount(\"/content/drive\")\n",
        "%cd '/content/drive/My Drive/Proj 1'\n",
        "\n",
        "# reading dataset\n",
        "data = pd.read_csv('biased_admit_dataset.csv')\n",
        "data=data[[\"x1\", \"x2\",'p',\"g\",\"admitted\"]] # changed\n",
        "data\n",
        "\n",
        "# converting to AIF360's dataset\n",
        "dataset = StandardDataset(data,protected_attribute_names=['g'],\n",
        "    privileged_classes=[['A']], categorical_features=[\"x1\",\"x2\"],\n",
        "    features_to_keep=['x1','x2','p'],label_name=\"admitted\", favorable_classes=[1]) # changed\n",
        "\n",
        "# Get the dataset and split into train and test\n",
        "dataset_orig_train, dataset_orig_test = dataset.split([0.7], shuffle=True)\n",
        "# scale the binary features\n",
        "scale_orig = StandardScaler()\n",
        "dataset_orig_train.features = scale_orig.fit_transform(dataset_orig_train.features)\n",
        "dataset_orig_test.features = scale_orig.transform(dataset_orig_test.features)\n",
        "# Metric for the original dataset\n",
        "privileged_groups = [{'g': \"A\"}]\n",
        "unprivileged_groups = [{'g':\"D\"}]\n",
        "\n",
        "# definig the train and test\n",
        "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "metric_orig_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "\n",
        "\n",
        "# Create a TensorFlow session\n",
        "sess = tf.Session()\n",
        "\n",
        "# Train an adversarial debiasing model\n",
        "adversarial_debiasing = AdversarialDebiasing(unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups,\n",
        "                                             scope_name='adversary',\n",
        "                                             sess=sess,\n",
        "                                             adversary_loss_weight=0.4,\n",
        "                                             batch_size=256\n",
        "                                             )\n",
        "adversarial_debiasing = adversarial_debiasing.fit(dataset_orig_train) # save this\n",
        "\n",
        "# Transform training data and align features\n",
        "dataset_transf_train = adversarial_debiasing.predict(dataset_orig_train)\n",
        "dataset_transf_test = adversarial_debiasing.predict(dataset_orig_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVY-4FiLStvl",
        "outputId": "da91f506-c82e-4458-84c3-660368c71fa6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: aif360 in /usr/local/lib/python3.8/dist-packages (0.5.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.8/dist-packages (from aif360) (1.2.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.8/dist-packages (from aif360) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from aif360) (3.5.3)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from aif360) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.8/dist-packages (from aif360) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.0->aif360) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.0->aif360) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0->aif360) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0->aif360) (3.1.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->aif360) (4.38.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->aif360) (23.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->aif360) (8.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->aif360) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->aif360) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->aif360) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->aif360) (1.15.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:No module named 'tempeh': LawSchoolGPADataset will be unavailable. To install, run:\n",
            "pip install 'aif360[LawSchoolGPA]'\n",
            "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
            "pip install 'aif360[Reductions]'\n",
            "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
            "pip install 'aif360[Reductions]'\n",
            "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
            "pip install 'aif360[Reductions]'\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Proj 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/aif360/metrics/utils.py:36: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  group_cond = np.logical_and(group_cond, X[:, index] == val)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0; iter: 0; batch classifier loss: 0.649110; batch adversarial loss: 0.767680\n",
            "epoch 1; iter: 0; batch classifier loss: 0.390120; batch adversarial loss: 0.809895\n",
            "epoch 2; iter: 0; batch classifier loss: 0.405202; batch adversarial loss: 0.905052\n",
            "epoch 3; iter: 0; batch classifier loss: 0.543127; batch adversarial loss: 0.973929\n",
            "epoch 4; iter: 0; batch classifier loss: 0.708691; batch adversarial loss: 1.003316\n",
            "epoch 5; iter: 0; batch classifier loss: 0.723572; batch adversarial loss: 0.953539\n",
            "epoch 6; iter: 0; batch classifier loss: 0.765089; batch adversarial loss: 0.936528\n",
            "epoch 7; iter: 0; batch classifier loss: 0.868509; batch adversarial loss: 0.945619\n",
            "epoch 8; iter: 0; batch classifier loss: 0.916315; batch adversarial loss: 0.912159\n",
            "epoch 9; iter: 0; batch classifier loss: 0.835036; batch adversarial loss: 0.903934\n",
            "epoch 10; iter: 0; batch classifier loss: 0.676049; batch adversarial loss: 0.848512\n",
            "epoch 11; iter: 0; batch classifier loss: 0.710928; batch adversarial loss: 0.828065\n",
            "epoch 12; iter: 0; batch classifier loss: 0.845719; batch adversarial loss: 0.841187\n",
            "epoch 13; iter: 0; batch classifier loss: 0.831167; batch adversarial loss: 0.845958\n",
            "epoch 14; iter: 0; batch classifier loss: 0.728880; batch adversarial loss: 0.813676\n",
            "epoch 15; iter: 0; batch classifier loss: 0.667019; batch adversarial loss: 0.782310\n",
            "epoch 16; iter: 0; batch classifier loss: 0.790059; batch adversarial loss: 0.804372\n",
            "epoch 17; iter: 0; batch classifier loss: 0.651462; batch adversarial loss: 0.787921\n",
            "epoch 18; iter: 0; batch classifier loss: 0.591494; batch adversarial loss: 0.768783\n",
            "epoch 19; iter: 0; batch classifier loss: 0.635872; batch adversarial loss: 0.775712\n",
            "epoch 20; iter: 0; batch classifier loss: 0.662589; batch adversarial loss: 0.782621\n",
            "epoch 21; iter: 0; batch classifier loss: 0.621201; batch adversarial loss: 0.779645\n",
            "epoch 22; iter: 0; batch classifier loss: 0.703413; batch adversarial loss: 0.779902\n",
            "epoch 23; iter: 0; batch classifier loss: 0.680443; batch adversarial loss: 0.761904\n",
            "epoch 24; iter: 0; batch classifier loss: 0.682729; batch adversarial loss: 0.724571\n",
            "epoch 25; iter: 0; batch classifier loss: 0.605943; batch adversarial loss: 0.742125\n",
            "epoch 26; iter: 0; batch classifier loss: 0.616917; batch adversarial loss: 0.739164\n",
            "epoch 27; iter: 0; batch classifier loss: 0.573078; batch adversarial loss: 0.739227\n",
            "epoch 28; iter: 0; batch classifier loss: 0.625333; batch adversarial loss: 0.718751\n",
            "epoch 29; iter: 0; batch classifier loss: 0.565781; batch adversarial loss: 0.714409\n",
            "epoch 30; iter: 0; batch classifier loss: 0.499585; batch adversarial loss: 0.713413\n",
            "epoch 31; iter: 0; batch classifier loss: 0.447933; batch adversarial loss: 0.713748\n",
            "epoch 32; iter: 0; batch classifier loss: 0.418543; batch adversarial loss: 0.700041\n",
            "epoch 33; iter: 0; batch classifier loss: 0.483715; batch adversarial loss: 0.695085\n",
            "epoch 34; iter: 0; batch classifier loss: 0.368709; batch adversarial loss: 0.691565\n",
            "epoch 35; iter: 0; batch classifier loss: 0.476364; batch adversarial loss: 0.681444\n",
            "epoch 36; iter: 0; batch classifier loss: 0.415660; batch adversarial loss: 0.684719\n",
            "epoch 37; iter: 0; batch classifier loss: 0.366834; batch adversarial loss: 0.673170\n",
            "epoch 38; iter: 0; batch classifier loss: 0.389933; batch adversarial loss: 0.669901\n",
            "epoch 39; iter: 0; batch classifier loss: 0.316298; batch adversarial loss: 0.672513\n",
            "epoch 40; iter: 0; batch classifier loss: 0.348262; batch adversarial loss: 0.678092\n",
            "epoch 41; iter: 0; batch classifier loss: 0.357943; batch adversarial loss: 0.666810\n",
            "epoch 42; iter: 0; batch classifier loss: 0.345279; batch adversarial loss: 0.683020\n",
            "epoch 43; iter: 0; batch classifier loss: 0.325723; batch adversarial loss: 0.684590\n",
            "epoch 44; iter: 0; batch classifier loss: 0.352254; batch adversarial loss: 0.679654\n",
            "epoch 45; iter: 0; batch classifier loss: 0.376634; batch adversarial loss: 0.683538\n",
            "epoch 46; iter: 0; batch classifier loss: 0.299220; batch adversarial loss: 0.687250\n",
            "epoch 47; iter: 0; batch classifier loss: 0.327187; batch adversarial loss: 0.676856\n",
            "epoch 48; iter: 0; batch classifier loss: 0.315124; batch adversarial loss: 0.678535\n",
            "epoch 49; iter: 0; batch classifier loss: 0.290046; batch adversarial loss: 0.683251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting back to dataframe"
      ],
      "metadata": {
        "id": "ZZ_duDdPJ7uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, y=(dataset_transf_train.convert_to_dataframe())\n",
        "df_test, y=(dataset_transf_test.convert_to_dataframe())"
      ],
      "metadata": {
        "id": "ZJli18b0_tw-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the logistic regression"
      ],
      "metadata": {
        "id": "_LjEokGeJ_PJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Fit a logistic regression model to the original training data\n",
        "lr_orig = LogisticRegression()\n",
        "lr_orig.fit(dataset_orig_train.features, dataset_orig_train.labels.ravel())\n",
        "\n",
        "y_pred_orig = lr_orig.predict(dataset_orig_test.features)\n",
        "score_function_values_original=lr_orig.decision_function(dataset_orig_test.features)\n",
        "\n",
        "\n",
        "# Fit a logistic regression model to the transformed training data\n",
        "lr_transf = LogisticRegression()\n",
        "lr_transf.fit(dataset_transf_train.features, dataset_transf_train.labels.ravel()) # save this.\n",
        "# Make predictions on the transformed test data\n",
        "y_pred_transf = lr_transf.predict(dataset_transf_test.features)  \n",
        "score_function_values_trans=lr_transf.decision_function(dataset_transf_test.features)\n",
        "\n",
        "# save the logistic regression weights. \n",
        "\n",
        "with open('lr_transf.pkl', 'wb') as f:\n",
        "    pickle.dump(lr_transf, f)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Compute metrics for the original model\n",
        "acc_orig = accuracy_score(dataset_orig_test.labels.ravel(), y_pred_orig)\n",
        "prec_orig = precision_score(dataset_orig_test.labels.ravel(), y_pred_orig)\n",
        "rec_orig = recall_score(dataset_orig_test.labels.ravel(), y_pred_orig)\n",
        "f1_orig = f1_score(dataset_orig_test.labels.ravel(), y_pred_orig)\n",
        "\n",
        "# Compute metrics for the transformed model\n",
        "acc_transf = accuracy_score(dataset_transf_test.labels.ravel(), y_pred_transf)\n",
        "prec_transf = precision_score(dataset_transf_test.labels.ravel(), y_pred_transf)\n",
        "rec_transf = recall_score(dataset_transf_test.labels.ravel(), y_pred_transf)\n",
        "f1_transf = f1_score(dataset_transf_test.labels.ravel(), y_pred_transf)\n",
        "\n",
        "# Print the computed metrics\n",
        "print(\"Original model:\")\n",
        "print(\"Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-score: {:.4f}\".format(acc_orig, prec_orig, rec_orig, f1_orig))\n",
        "print(\"\\nTransformed model:\")\n",
        "print(\"Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-score: {:.4f}\".format(acc_transf, prec_transf, rec_transf, f1_transf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU2weRsIFQXg",
        "outputId": "a10178f7-0de5-4294-b02e-3b944f601367"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model:\n",
            "Accuracy: 0.9077, Precision: 0.9412, Recall: 0.7298, F1-score: 0.8221\n",
            "\n",
            "Transformed model:\n",
            "Accuracy: 0.9957, Precision: 1.0000, Recall: 0.9207, F1-score: 0.9587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ARXNFcAjJVJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating statistical parity"
      ],
      "metadata": {
        "id": "NjxOGz_QJVi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test[\"predicted\"]=y_pred_transf\n",
        "def calculate_statistical_parity(df):\n",
        "  # calculate proportion of positive outcomes for sensitive group\n",
        "  priviledge_group = df[df['g'] == 1]\n",
        "  priviledge_prop = priviledge_group['predicted'].mean()\n",
        "\n",
        "  # calculate proportion of positive outcomes for non-sensitive group\n",
        "  non_priviledge_group = df[df['g'] == 0]\n",
        "  non_priviledge_prop = non_priviledge_group['predicted'].mean()\n",
        "\n",
        "  # calculate difference between the two proportions\n",
        "  statistical_parity = priviledge_prop - non_priviledge_prop\n",
        "\n",
        "  return('Statistical parity: {}'.format (statistical_parity))\n",
        "\n",
        "print(\"After adverserial debiasing,\", calculate_statistical_parity(df_test))\n",
        "\n",
        "df_test_org, y=(dataset_orig_test.convert_to_dataframe())\n",
        "df_test_org[\"predicted\"]=list(y_pred_orig)\n",
        "print(\"Before adverserial debiasing\", calculate_statistical_parity(df_test_org))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5WvfIlIisrT",
        "outputId": "86b50b22-e843-4747-f485-12cdb7664e8b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After adverserial debiasing, Statistical parity: -0.09266666666666666\n",
            "Before adverserial debiasing Statistical parity: 0.3506666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# calculating equalized odds"
      ],
      "metadata": {
        "id": "BH-Q3ScSJZO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "def fnr_and_fpr(df):\n",
        "  y_true,y_pred=df[\"admitted\"],df[\"predicted\"]\n",
        "  # Assuming y_true and y_pred are arrays of true and predicted labels, respectively\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  # Extract the TN, FP, FN, TP values from the confusion matrix\n",
        "  tn = cm[0, 0]\n",
        "  fp = cm[0, 1]\n",
        "  fn = cm[1, 0]\n",
        "  tp = cm[1, 1]\n",
        "  tpr = tp / (tp + fn)\n",
        "  fpr = fp / (fp + tn)\n",
        "  return(\"TPR {}, FPR {}\".format(tpr, fpr))\n",
        "\n",
        "priviledge_orginal=df_test_org[df_test_org['g'] == 1]\n",
        "non_priviledge_original=df_test_org[df_test_org['g'] == 0]\n",
        "print(\"BEFORE: priviledge: {}, non-prvilege: {}\".format(fnr_and_fpr(priviledge_orginal),fnr_and_fpr(non_priviledge_original)))\n",
        "\n",
        "priviledge_trans=df_test[df_test['g'] == 1]\n",
        "non_priviledge_trans=df_test[df_test['g'] == 0]\n",
        "print(\"AFTER: priviledge: {}, non-prvilege: {}\".format(fnr_and_fpr(non_priviledge_trans), fnr_and_fpr(priviledge_trans)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rf0E3lPx5zC",
        "outputId": "624dcba1-0a98-47da-8fbb-6f8f1c58f557"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEFORE: priviledge: TPR 0.8141210374639769, FPR 0.04714640198511166, non-prvilege: TPR 0.4098360655737705, FPR 0.0015186028853454822\n",
            "AFTER: priviledge: TPR 0.9294871794871795, FPR 0.0, non-prvilege: TPR 0.75, FPR 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the differences to be the equalized odds\n",
        "\n",
        "## TPR \n",
        "### Before debiasing \n",
        "print(\"TPR diff Before debiasing\",(0.8141210374639769-0.4098360655737705))\n",
        "### After debiasing\n",
        "print(\"TPR diff after debiasing\",(0.9294871794871795-0.75))\n",
        "\n",
        "\n",
        "## FPR\n",
        "## Before debiasing\n",
        "print(\"FPR diff before debiasing\",(0.04714640198511166-0.0015186028853454822))\n",
        "## After debiasing\n",
        "print(\"FPR diff after debiasing\",(0-0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O78FDia8JerK",
        "outputId": "3e5db3b8-358c-4155-bf72-18f5c62dad52"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPR diff Before debiasing 0.4042849718902064\n",
            "TPR diff after debiasing 0.17948717948717952\n",
            "FPR diff before debiasing 0.04562779909976618\n",
            "FPR diff after debiasing 0\n"
          ]
        }
      ]
    }
  ]
}